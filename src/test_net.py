"""
#############################################################################################################

Test routines to evaluate performance of model

    Alice   2022

#############################################################################################################
"""

import  os
import  numpy               as np
import  tensorflow          as tf

from    math                import pi, sin, cos, atan, atan2
from    PIL                 import Image
from    skimage.metrics     import structural_similarity    as ssim
from    skimage.metrics     import mean_squared_error       as mse
from    tensorflow.keras    import losses
from    img_utils           import array_to_image, save_collage, save_animation, save_image, draw_angle

import  data_gen


save_dir    = None          # should be set by the main

last_frac   = 0.1           # fraction of last frames over all, for similarity metrics
resize      = ( 256, 256 )  # size of the image for display
DEBUG0      = False


# ===========================================================================================================
#
#   Predictions from model
#
#   - pred_one
#   - pred_tset
#
# ===========================================================================================================


def pred_one( model, data, idx ):
    """ -----------------------------------------------------------------------------------------------------
    Compute the predictions for a single sample

    model:          [keras.engine.training.Model]
    data:           [dic] dataset with keys 'x' and 'y'
    idx:            [int] index of the sample in the dataset
    ----------------------------------------------------------------------------------------------------- """
    x1, x2          = data[ 'x' ]
    i1              = x1[ idx ][ np.newaxis, ... ]
    i2              = x2[ idx ][ np.newaxis, ... ]
    pred            = model( [ i1, i2 ] )               # pred is a list with two tensors ( 1, 64, 64, 1 )
    pred            = [ p.numpy() for p in pred ]
    pred            = [ p.squeeze() for p in pred ]     # eliminate batch and channel dimensions
    return pred


def pred_tset( model, data, batch=128 ):
    """ -----------------------------------------------------------------------------------------------------
    Compute the predictions over a dataset

    model:          [keras.engine.training.Model]
    data:           [dic] dataset with keys 'x' and 'y'
    batch:          [int] batch size to use
    ----------------------------------------------------------------------------------------------------- """
    x1, x2          = data[ 'x' ]
    y1              = np.zeros( x1.shape, dtype=x1.dtype )
    y2              = np.zeros( x1.shape, dtype=x1.dtype )

    for i0 in range( 0, x1.shape[ 0 ], batch ):
        i1             = i0 + batch
        pred           = model( [ x1[ i0 : i1 ], x2[ i0 : i1 ] ] )  # pred: list of two tensors ( batch, 64, 64, 1 )
        y1[ i0 : i1 ]  = pred[ 0 ]
        y2[ i0 : i1 ]  = pred[ 1 ]

    return y1, y2


def pred_onerot_tset( model, data ):
    """ -----------------------------------------------------------------------------------------------------
    Compute the predictions over a dataset with one rotate image only, for all rotations
    like those generated by data_gen.gen_onerot_tset()

    model:          [keras.engine.training.Model] encoder only model
    data:           [list] of np.arrays of shape ( n_rot, *isize )
    ----------------------------------------------------------------------------------------------------- """
    lat     = []
    for d in data:
        pred    = model( d )  # pred: tensor with shape ( n_rot, n_rot )
        lat.append( np.array( pred ) )

    return lat


def pred_invert_one( model, data, idx ):
    """ -----------------------------------------------------------------------------------------------------
    Compute the angle estimation for a single sample, using a simple "inverted" model
    just the encoder part is used, twice, for the first and second (rotated) image,
    then the shift matrix that, when applied to the ffirst latent, better approximate the
    second latent, is taken as corresponding to the rotation angle

    model:          [keras.engine.training.Model]
    data:           [dic] dataset with keys 'x' and 'y'
    idx:            [int] index of the sample in the dataset

    return:         [float] angle [degrees]
    ----------------------------------------------------------------------------------------------------- """
    x1, x2          = data[ 'y' ]                   # the target in the dataset is made by two images
    i1              = x1[ idx ][ np.newaxis, ... ]
    i2              = x2[ idx ][ np.newaxis, ... ]
    latent1         = model( i1 )[ 0 ]              # encode the latent for the first image
    latent2         = model( i2 )[ 0 ]              # encode the latent for the second image
    shift_mats      = data_gen.shift_mat[ 1 : ]     # take all shifting matrices, except the first one
    # apply all matrices to the first latent in predicting the second one
    pred_latent2    = [ tf.tensordot( m, latent1, axes=1  )       for m in shift_mats ]
    # compute the similarity between prediceted and current second latent
    pred_err        = [ losses.cosine_similarity( p, latent2 )    for p in pred_latent2 ]
    pred_err        = np.array( pred_err )
    i_rot           = pred_err.argmin()             # take the index of the closest prediction
    # and convert it in degrees
    angle           = ( 1 + i_rot ) * 360 / data_gen.n_rot
    angle           = angle % 360
    return angle


def pred_baseline_one( model, data, idx ):
    """ -----------------------------------------------------------------------------------------------------
    Compute the angle estimation for a single sample, using a baseline model

    model:          [keras.engine.training.Model]
    data:           [dic] dataset with keys 'x' and 'y'
    idx:            [int] index of the sample in the dataset

    return:         [float] angle [degrees]
    ----------------------------------------------------------------------------------------------------- """
    x1, x2          = data[ 'y' ]                   # the target in the dataset is made by two images
    i1              = x1[ idx ][ np.newaxis, ... ]
    i2              = x2[ idx ][ np.newaxis, ... ]
    sin_cos         = model( [ i1, i2 ] )[ 0 ]      # predict as [ sin, cos ] vector
    sin_cos         = sin_cos.numpy()
    angle           = np.arctan2( *sin_cos )
    angle           = 180 * angle / pi
    """
    # used for one-hot vector output
    one_hot         = model( [ i1, i2 ] )[ 0 ]      # predict as one-hot vector
    one_hot         = one_hot.numpy()
    i_rot           = one_hot.argmax()              # take the index of the predicted angle
    # and convert it in degrees
    angle           = ( 1 + i_rot ) * 360 / data_gen.n_rot
    angle           = angle % 360
    """
    return angle



# ===========================================================================================================
#
#   Display prediction
#
#   - show_tset_one
#   - show_invert_one
#
# ===========================================================================================================

def show_tset_one_color( model, data, idx ):
    """ -----------------------------------------------------------------------------------------------------
    Plot a collage of the predicted images compared with the target images

    model:          [keras.engine.training.Model]
    data:           [dic] dataset with keys 'x' and 'y'
    idx:            [int] index of the sample in the dataset
    ----------------------------------------------------------------------------------------------------- """

    pred        = pred_one( model, data, idx )
    targ        = [ t[ idx ] for t in data[ 'y' ] ]

    pred        = [ array_to_image( p, rgb=True ) for p in pred ]     # predicted images in PIL
    targ        = [ array_to_image( t, rgb=True ) for t in targ ]

    fmt         = "res_{:05d}.{}"
    images      = ( pred, targ )
    fname       = os.path.join( save_dir, fmt.format( idx, 'jpg' ) )
    save_collage( images, *resize, fname )


def show_tset_one_bw( model, data, idx, threshold=0.3 ):
    """ -----------------------------------------------------------------------------------------------------
    Plot a collage of the predicted images compared with the target images

    model:          [keras.engine.training.Model]
    data:           [dic] dataset with keys 'x' and 'y'
    idx:            [int] index of the sample in the dataset
    threshold:      [float] for binarizing the images
    ----------------------------------------------------------------------------------------------------- """

    pred        = pred_one( model, data, idx )
    targ        = [ t[ idx ] for t in data[ 'y' ] ]

    pred_t      = [ p > threshold for p in pred ]           # binarize the image
    pred        = [ array_to_image( p ) for p in pred ]     # predicted images in PIL
    pred_t      = [ array_to_image( 1. * p ) for p in pred_t ]  # predicted images in PIL
    targ        = [ t.squeeze()   for t in targ ]
    targ_t      = [ t > threshold for t in targ ]
    targ        = [ array_to_image( t ) for t in targ ]
    targ_t      = [ array_to_image( 1. * t ) for t in targ_t ]

    fmt         = "res_{:05d}.{}"
    images      = ( pred, targ )
    fname       = os.path.join( save_dir, fmt.format( idx, 'jpg' ) )
    save_collage( images, *resize, fname )

    fmt         = "res_{:05d}t.{}"
    images      = ( pred_t, targ_t )
    fname       = os.path.join( save_dir, fmt.format( idx, 'jpg' ) )
    save_collage( images, *resize, fname )


def show_invert_one( model, data, descr, idx, internal=False ):
    """ -----------------------------------------------------------------------------------------------------
    Plot a collage of the angle predicted by the inverted model compared with the true one

    model:          [keras.engine.training.Model]
    data:           [dic] dataset with keys 'x' and 'y'
    descr;          [list] description of dataset samples
    idx:            [int] index of the sample in the dataset
    internal:       [bool] True for internal images, with opposite angle convention w.r.t. external
    ----------------------------------------------------------------------------------------------------- """

    inp1            = data[ 'y' ][ 0 ][ idx ]
    inp2            = data[ 'y' ][ 1 ][ idx ]
    _, r1, r2, _    = descr[ idx ]
    angle1          = r1 * 360 / data_gen.n_rot
    angle2          = r2 * 360 / data_gen.n_rot
    pred_angle      = pred_invert_one( model, data, idx )
    if internal:
        angle1      = 360 - angle1
        angle2      = 360 - angle2
        pred_angle  = 360 - pred_angle
    pred_angle2     = ( angle1 + pred_angle ) % 360
    if pred_angle2 > 360:
        pred_angle2 = pred_angle2 - 360
#   print( r1, r2, angle1, angle2, pred_angle, pred_angle2 )

    img1            = draw_angle( inp1, angle1, resize=resize )
    img2            = draw_angle( inp2, angle1, angle2, pred_angle2, resize=resize )

    fmt             = "angle_{:05d}.{}"
    images          = [ [ img1, img2 ] ]
    fname           = os.path.join( save_dir, fmt.format( idx, 'jpg' ) )
    save_collage( images, *resize, fname )


def show_latents( model, obj, dataset, include_imgs=True ):
    """ -----------------------------------------------------------------------------------------------------
    display as image the latent vector for an objects, and all its shifts

    model:          [keras.engine.training.Model]
    obj:            [str] name of an object
    dataset         [str] dataset code, see load_cnfg.py for details of the currently available dataset codes
    ----------------------------------------------------------------------------------------------------- """

    encoder         = model.encoder.model
    decoder         = model.decoder.model
    img             = data_gen.read_generic( obj, 0, dataset )
    img             = img[ np.newaxis, ... ]
    latent1         = encoder( img )[ 0 ]           # encode the latent for the image at 0 rotation
    if len( data_gen.shift_mat ) == 0:              # ensure shift_mat in data_gen has been validated
        data_gen.shifts()
    # apply all matrices to the latent in generating shifts
    latents         = [ tf.tensordot( m, latent1, axes=1  )       for m in data_gen.shift_mat ]
    if include_imgs:
        in_img  = [ data_gen.read_generic( obj, r, dataset ) for r in range( data_gen.n_rot ) ]
        out_img = [ decoder( l[ np.newaxis, ... ] )[ 0 ] for l in latents ]

    out_dir         = os.path.join( save_dir, obj )
    if not os.path.isdir( out_dir ):
        os.makedirs( out_dir )

    fmt             = "latent_{:03d}.{}"
    resize          = ( 32, 4 * data_gen.n_rot )    # make the image visible
    for i, l in enumerate( latents ):
        image       = l.numpy()
        fname       = os.path.join( out_dir, fmt.format( i, 'jpg' ) )
        save_image( image, fname, resize=resize, transpose=True )
        if include_imgs:
            fname       = os.path.join( out_dir, f"img_in_{i:03d}.jpg" )
            save_image( in_img[ i ], fname, rgb=True, resize=( 256, 256 ) )
            fname       = os.path.join( out_dir, f"img_out_{i:03d}.jpg" )
            save_image( out_img[ i ].numpy(), fname, rgb=True, resize=( 256, 256 ) )


    



# ===========================================================================================================
#
#   metrics
#
#   - iou
#   - metric_tset
#
# ===========================================================================================================

def angle_diff( pred, targ ):
    """ ---------------------------------------------------------------------------------------------------------
    compute the error in angle estimation, one over 2 Pi, and another over Pi

    pred:           [float] predicted angle [degrees]
    targ:           [float] target angle [degrees]

    return:         [tuple] ( error over 2 Pi, error over Pi ) [0..1]
    --------------------------------------------------------------------------------------------------------- """
    d_angle = targ - pred
    d_angle = pi * d_angle / 180
    s       = sin( d_angle )
    c       = cos( d_angle )
    diff    = ( atan2( s, c ) / pi ) ** 2
    diff_pi = s ** 2
    """
    # this is the measure using arctan, that has a sharp increase in error around pi/2
    # is seems more linear the measure with sin^2
    diff_pi = ( 2 * atan( s / c ) / pi ) ** 2
    """

    return diff, diff_pi


def iou( pred, targ, threshold=0.3 ):
    """ ---------------------------------------------------------------------------------------------------------
    compute intersection over union

    pred:           [np.ndarray] predicted image (graylevel)
    targ:           [np.ndarray] target image (graylevel)
    --------------------------------------------------------------------------------------------------------- """
    pred    = pred > threshold                              # binarize the arrays
    targ    = targ > threshold

    if np.sum( pred ) < 2:
        return 0.0
    if np.sum( targ ) < 2:
        return 0.0

    inter   = np.logical_and( targ, pred )
    union   = np.logical_or( targ, pred )
    s_inter = np.sum( inter )                               # number of pixels in the intersection
    s_union = np.sum( union )                               # number of pixels in the union

    return ( s_inter / s_union )


def metric_tset_bw( model, dset, desc ):
    """ -----------------------------------------------------------------------------------------------------
    assess how close the predictions are to the targets, using several metrics, valid only for graylevel images

    model:          [keras.engine.training.Model]
    dset:           [dic] dataset with keys 'x' and 'y'
    desc:           [list] description of each sample in the dataset

    NOTE: elements of desc should be ( obj, rot1, rot2, transl ) and the number of objects in the validation
    set are computed by the number of different obj names in desc
    ----------------------------------------------------------------------------------------------------- """

    few         = 5                         # the few worst and best cases to show
    n           = len( desc )
    fname       = os.path.join( save_dir, "metrics.txt" )
    f           = open( fname, 'w' )
    f.write( "metrics on the test dataset with {} samples\n".format( n ) )

    ssims       = np.zeros( n )             # structural similarity
    mses        = np.zeros( n )             # mean square error
    ious        = np.zeros( n )             # IoU
    a_ssims     = np.zeros( n )             # autoencoder structural similarity
    a_mses      = np.zeros( n )             # autoencoder mean square error
    a_ious      = np.zeros( n )             # autoencoder IoU
    d1, d2      = dset[ 'y' ]
    p1, p2      = pred_tset( model, dset )

    for i, d in enumerate( desc ):
        y1              = p1[ i ]
        y2              = p2[ i ]
        t1              = d1[ i ]
        t2              = d2[ i ]
        ssims[ i ]      = ssim( t2.squeeze(), y2.squeeze() )
        mses[ i ]       = mse( t2.squeeze(), y2.squeeze() )
        ious[ i ]       = iou( t2.squeeze(), y2.squeeze() )
        a_ssims[ i ]    = ssim( t1.squeeze(), y1.squeeze() )
        a_mses[ i ]     = mse( t1.squeeze(), y1.squeeze() )
        a_ious[ i ]     = iou( t1.squeeze(), y1.squeeze() )

    objects         = list( set( [ d[ 0 ] for d in desc ] ) )  # classes of objects in the dataset
    objects.sort()                                              # important to ensure same order of coil_o_classes
    n_objects       = len( objects )                            # number of objects
    smpl_objs       = n // n_objects                            # number of samples for one object

    i0              = 0                                         # first index of a dataset type
    for obj in objects:
        i1          = i0 + smpl_objs                            # last index of a dataset type
        ssims_p     = ssims[ i0 : i1 ]
        mses_p      = mses[ i0 : i1 ]
        ious_p      = ious[ i0 : i1 ]
        a_ssims_p   = a_ssims[ i0 : i1 ]
        a_mses_p    = a_mses[ i0 : i1 ]
        a_ious_p    = a_ious[ i0 : i1 ]

        argsort     = np.argsort( ious_p )        # sort by IoU

        f.write( "---------------------------------------------------------------------------------------\n" )
        f.write( "object type " + obj + "\n" )
        f.write( "---------------------------------------------------------------------------------------\n" )
        f.write( "idx   r1 r2 tr          rotation                   autoencoder\n" )
        f.write( "                     msqerr  strsim  IoU      msqerr  strsim  IoU\n" )
        f.write( "---------------------------------------------------------------------------------------\n" )
        fmt     = "{:05d} {:03d}  {:03d}  {:02d}    {:7.4f} {:7.4f} {:7.4f}   {:7.4f} {:7.4f} {:7.4f}\n"
        for s in range( few ):
            i       = argsort[ s ]
            p       = desc[ i + i0 ][ 1 : ]         # parameters of rotations and translation
            f.write( fmt.format( i+i0, *p, mses_p[i], ssims_p[i], ious_p[i], a_mses_p[i], a_ssims_p[i], a_ious_p[i] ) )

        f.write( "\n    ...\n\n" )

        for s in range( few ):
            i   = argsort[ -s -1 ]
            p       = desc[ i + i0 ][ 1 : ]         # parameters of rotations and translation
            f.write( fmt.format( i+i0, *p, mses_p[i], ssims_p[i], ious_p[i], a_mses_p[i], a_ssims_p[i], a_ious_p[i] ) )
        f.write( '\n' )

        i0          = i1

    f.write( '\n' )
    f.write( "---------------------------------------------------------------------------------------\n" )
    fmt     = "mean             {:7.4f}  {:7.4f}  {:7.4f} {:7.4f}  {:7.4f}  {:7.4f}\n"
    f.write( fmt.format( mses.mean(), ssims.mean(), ious.mean(), a_mses.mean(), a_ssims.mean(), a_ious.mean() ) )
    f.write( "---------------------------------------------------------------------------------------\n" )
    f.close()


def metric_tset_color( model, dset, desc ):
    """ -----------------------------------------------------------------------------------------------------
    assess how close the predictions are to the targets, using several metrics possible for color images

    model:          [keras.engine.training.Model]
    dset:           [dic] dataset with keys 'x' and 'y'
    desc:           [list] description of each sample in the dataset

    NOTE: elements of desc should be ( obj, rot1, rot2, transl ) and the number of objects in the validation
    set are computed by the number of different obj names in desc
    ----------------------------------------------------------------------------------------------------- """

    few         = 5                         # the few worst and best cases to show
    n           = len( desc )
    fname       = os.path.join( save_dir, "metrics.txt" )
    f           = open( fname, 'w' )
    f.write( "metrics on the test dataset with {} samples\n".format( n ) )

    ssims       = np.zeros( n )             # structural similarity
    mses        = np.zeros( n )             # mean square error
    a_ssims     = np.zeros( n )             # autoencoder structural similarity
    a_mses      = np.zeros( n )             # autoencoder mean square error
    d1, d2      = dset[ 'y' ]
    p1, p2      = pred_tset( model, dset )

    for i, d in enumerate( desc ):
        y1              = p1[ i ]
        y2              = p2[ i ]
        t1              = d1[ i ]
        t2              = d2[ i ]
        ssims[ i ]      = ssim( t2.squeeze(), y2.squeeze(), channel_axis=2 )
        mses[ i ]       = mse( t2.squeeze(), y2.squeeze() )
        a_ssims[ i ]    = ssim( t1.squeeze(), y1.squeeze(), channel_axis=2 )
        a_mses[ i ]     = mse( t1.squeeze(), y1.squeeze() )

    objects         = list( set( [ d[ 0 ] for d in desc ] ) )  # classes of objects in the dataset
    objects.sort()                                              # important to ensure same order of coil_o_classes
    n_objects       = len( objects )                            # number of objects
    smpl_objs       = n // n_objects                            # number of samples for one object

    i0              = 0                                         # first index of a dataset type
    for obj in objects:
        i1          = i0 + smpl_objs                            # last index of a dataset type
        ssims_p     = ssims[ i0 : i1 ]
        mses_p      = mses[ i0 : i1 ]
        a_ssims_p   = a_ssims[ i0 : i1 ]
        a_mses_p    = a_mses[ i0 : i1 ]

        argsort     = np.argsort( mses_p )        # sort by MSE

        f.write( "---------------------------------------------------------------------------------------\n" )
        f.write( "object type " + obj + "\n" )
        f.write( "---------------------------------------------------------------------------------------\n" )
        f.write( "idx   r1 r2 tr          rotation          autoencoder\n" )
        f.write( "                     msqerr  strsim      msqerr  strsim\n" )
        f.write( "---------------------------------------------------------------------------------------\n" )
        fmt     = "{:05d} {:03d}  {:03d}  {:02d}    {:7.4f} {:7.4f}   {:7.4f} {:7.4f}\n"
        for s in range( few ):
            i       = argsort[ s ]
            p       = desc[ i + i0 ][ 1 : ]         # parameters of rotations and translation
            f.write( fmt.format( i+i0, *p, mses_p[i], ssims_p[i], a_mses_p[i], a_ssims_p[i], ) )

        f.write( "\n    ...\n\n" )

        for s in range( few ):
            i   = argsort[ -s -1 ]
            p       = desc[ i + i0 ][ 1 : ]         # parameters of rotations and translation
            f.write( fmt.format( i+i0, *p, mses_p[i], ssims_p[i], a_mses_p[i], a_ssims_p[i], ) )
        f.write( '\n' )

        i0          = i1

        f.write( '\n' )
        f.write( "---------------------------------------------------------------------------------------\n" )
        fmt     = "class mean       {:7.4f}  {:7.4f}  {:7.4f}  {:7.4f}\n"
        f.write( fmt.format( mses_p.mean(), ssims_p.mean(), a_mses_p.mean(), a_ssims_p.mean(), ) )
        f.write( "---------------------------------------------------------------------------------------\n\n" )

    f.write( '\n\n' )
    f.write( "---------------------------------------------------------------------------------------\n" )
    fmt     = "all classes mean {:7.4f}  {:7.4f}  {:7.4f}  {:7.4f}\n"
    f.write( fmt.format( mses.mean(), ssims.mean(), a_mses.mean(), a_ssims.mean(), ) )
    f.write( "---------------------------------------------------------------------------------------\n" )
    f.close()


def metric_inverted( model, dset, desc ):
    """ -----------------------------------------------------------------------------------------------------
    assess how close the predictions are to the targets, using several metrics possible for color images

    model:          [keras.engine.training.Model]
    dset:           [dic] dataset with keys 'x' and 'y'
    desc:           [list] description of each sample in the dataset

    NOTE: elements of desc should be ( obj, rot1, rot2, transl ) and the number of objects in the validation
    set are computed by the number of different obj names in desc
    ----------------------------------------------------------------------------------------------------- """

    few         = 5                         # the few worst and best cases to show
    n           = len( desc )
    fname       = os.path.join( save_dir, "inverted.txt" )
    f           = open( fname, 'w' )
    f.write( "inverted metrics on the test dataset with {} samples\n".format( n ) )

    a_true      = np.zeros( n )             # true angle [degrees]
    a_pred      = np.zeros( n )             # predicted angle [degrees]
    a_diff      = np.zeros( n )             # difference over 2 Pi
    a_diff_pi   = np.zeros( n )             # difference over Pi

    for i, d in enumerate( desc ):
        _, r1, r2, _    = d
        angle           = ( r2 - r1 ) * 360 / data_gen.n_rot
        if angle < 0:
            angle   = 360 + angle
        if data_gen.bsline:
            pred_angle      = pred_baseline_one( model, dset, i )
        else:
            pred_angle      = pred_invert_one( model, dset, i )
        a_true[ i ]     = angle
        a_pred[ i ]     = pred_angle
        diff            = angle_diff( pred_angle, angle )
        a_diff[ i ]     = diff[ 0 ]
        a_diff_pi[ i ]  = diff[ 1 ]

    objects         = list( set( [ d[ 0 ] for d in desc ] ) )  # classes of objects in the dataset
    objects.sort()                                              # important to ensure same order of coil_t_classes
    n_objects       = len( objects )                            # number of objects
    smpl_objs       = n // n_objects                            # number of samples for one object

    i0              = 0                                         # first index of a dataset type
    for obj in objects:
        i1          = i0 + smpl_objs                            # last index of a dataset type
        a_true_p    = a_true[ i0 : i1 ]
        a_pred_p    = a_pred[ i0 : i1 ]
        a_diff_p    = a_diff[ i0 : i1 ]
        a_diff_pi_p = a_diff_pi[ i0 : i1 ]

        argsort     = np.argsort( a_diff_pi_p )        # sort by difference over Pi

        f.write( "----------------------------------------------------\n" )
        f.write( "object type " + obj + "\n" )
        f.write( "----------------------------------------------------\n" )
        f.write( "idx   r1  r2     rotation       error\n" )
        f.write( "               true  pred   over 2PI  over PI\n" )
        f.write( "----------------------------------------------------\n" )
        fmt     = "{:05d} {:03d}  {:03d}   {:4.0f} {:4.0f}   {:7.4f} {:7.4f}\n"
        for s in range( few ):
            i       = argsort[ s ]
            p       = desc[ i + i0 ][ 1 : -1 ]         # parameters of rotations
            f.write( fmt.format( i+i0, *p, a_true_p[i], a_pred_p[i], a_diff_p[i], a_diff_pi_p[i], ) )

        f.write( "\n    ...\n\n" )

        for s in range( few ):
            i   = argsort[ -s -1 ]
            p       = desc[ i + i0 ][ 1 : -1 ]         # parameters of rotations
            f.write( fmt.format( i+i0, *p, a_true_p[i], a_pred_p[i], a_diff_p[i], a_diff_pi_p[i], ) )
        f.write( '\n' )

        f.write( '\n' )
        f.write( "----------------------------------------------------\n" )
        fmt     = "class mean                 {:7.4f}  {:7.4f}\n"
        f.write( fmt.format( a_diff_p.mean(), a_diff_pi_p.mean() ) )
        f.write( "----------------------------------------------------\n\n" )

        i0          = i1

    f.write( '\n\n' )
    f.write( "----------------------------------------------------\n" )
    fmt     = "all classes mean           {:7.4f}  {:7.4f}\n"
    f.write( fmt.format( a_diff.mean(), a_diff_pi.mean() ) )
    f.write( "----------------------------------------------------\n" )
    f.close()

    



# ===========================================================================================================
#
#   latent analysis
#
#   - latent_coding
#   - latent_mean_coding
#
# ===========================================================================================================

def latent_coding( lat, objs ):
    """ ---------------------------------------------------------------------------------------------------------
    compute the "coding", i.e. the neurons in the latent that better code for one objects than all the others,
    computed separatedly for each angle. The final result is a map n_rot x n_rot, with latents at different angles
    stacked vertically

    lat:            [list] of predicted latents, each element is for an object, with shape ( n_rot, n_rot )
    objs:           [list] of object names

    return:         [list] latent coding for each object, with one latent per angle
    --------------------------------------------------------------------------------------------------------- """

    lat_code    = []
    n_obj       = len( lat )
    fmt         = "lat_angle_{}.{}"

    for i in range( n_obj ):
        others  = [ lat[ j ] for j in range( n_obj ) if j != i ]
        others  = np.array( others )
        mplus   = others.mean( axis=0 ) + others.std( axis=0 )
        code    = 1. * ( lat[ i ] > mplus )
        lat_code.append( code )

    for l, o in zip( lat_code, objs ):
        fname       = os.path.join( save_dir, fmt.format( o, 'png' ) )
        save_image( l, fname )

    return lat_code


def latent_mean_coding( lat, objs, code_length=8 ):
    """ ---------------------------------------------------------------------------------------------------------
    compute the "coding", i.e. the neurons in the latent that better code for one objects than all the others,
    averaged over all angles. The final result is a map n_obj x n_rot, with latents of different objects
    stacked vertically

    basically the score for a neuron to count as coding, is
                         m1 - m2
          z = ---------------------------------
                Sqrt( s1^2 / N1 + s2^2 / N2 )

    and all neurons are ranked for this score, and code_length neurons are taken as coding, with the additional
    constraint of being z > 0


    lat:            [list] of predicted latents, each element is for an object, with shape ( n_rot, n_rot )
    objs:           [list] of object names

    return:         [list] latent coding for all objects
    --------------------------------------------------------------------------------------------------------- """

    lat_code    = []
    n_obj       = len( lat )
    fmt         = "lat_mean.{}"

    for i in range( n_obj ):
        others      = [ lat[ j ] for j in range( n_obj ) if j != i ]
        others      = np.array( others )
        mothers     = others.mean( axis=(0,1) )
        sothers     = others.std( axis=(0,1) )
        m           = lat[ i ].mean( axis=0 )
        s           = lat[ i ].std( axis=0 )
        mm          = m - mothers
        ss          = sothers + s
        sel         = np.divide ( mm, ss, out=np.zeros_like( mm ), where=ss!=0 )
        rank        = np.flipud( sel.argsort( axis=None ) )
        idx         = rank[ : code_length ]
        code        = np.zeros( sel.shape[ -1 ] )
        code[ idx ] = 1.0
        code        = code * ( sel > 0 )
        lat_code.append( code )

    lat_code    = np.array( lat_code )
    fname       = os.path.join( save_dir, fmt.format( 'png' ) )
    save_image( lat_code, fname )

    return lat_code
